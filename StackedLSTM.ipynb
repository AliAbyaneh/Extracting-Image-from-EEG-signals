{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZBaFQuM51cf"
   },
   "source": [
    "This Code is designed to be used in Google Colab and Google Cloud Environment by Ali Abyaneh.\n",
    "**Bachelor Thesis, Extracting Images From EEG signals.**\n",
    "\n",
    "The following code attempts to classify an EEG dataset, which is *The \"MNIST\" of Brain Digits*, into three categories, namely, first digits(0-9), second digits(0-9), and noise which is a digit observed by the observor other than first two digits. The goal and results are elucidated in the readme at master branch of this repository.\n",
    "\n",
    "Feel free to use this code if it can be helpful for you!\n",
    "\n",
    "*Author : Ali Abyaneh*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was cleared above, this code is written to be used on google colab, so in the follwoing section we add google drive to the google colab so as to use the dataset which is uploaded in my gdrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KC0DUVsO5ucT"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "import os\n",
    "import zipfile\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "os.chdir(\"../../\")\n",
    "os.chdir(\"../gdrive/My Drive\")\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to import neccessary libraies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VO_wgfT_51l0"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "!pip install scipy\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_vjkaoN523M"
   },
   "source": [
    "In the following cell, we read the dataset, that is, *\"EP1.01.txt\"*, and parse it into different categories. Data is selected from 14 different location of the brain, the graph of which is shown below.\n",
    "\n",
    "![epoc-20-10.jpg](epoc-20-10.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_I7f17Xi52_9"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "EPOC_Brain_location = {\"AF3\" : 0,\n",
    "                  \"F7\"  : 1,\n",
    "                  \"F3\"  : 2,\n",
    "                  \"FC5\" : 3,\n",
    "                  \"T7\"  : 4,\n",
    "                  \"P7\"  : 5,\n",
    "                  \"O1\"  : 6,\n",
    "                  \"O2\"  : 7,\n",
    "                  \"P8\"  : 8, \n",
    "                  \"T8\"  : 9,\n",
    "                  \"FC6\" : 10,\n",
    "                  \"F4\"  : 11,\n",
    "                  \"F8\"  : 12,\n",
    "                  \"AF4\" : 13}\n",
    "MW_Brain_location = {\"FP1\" : 0}\n",
    "N_Samples_device = {\"MindWave\" : 1024,\n",
    "          \"EPOC\"     : 256 \n",
    "          }\n",
    "def Load_data(infile, device, data_size = 100, event_start_point = 67635, split_data = False):\n",
    "    if device == \"EPOC\":\n",
    "        Brain_location = EPOC_Brain_location\n",
    "    elif device == \"MindWave\":\n",
    "        Brain_location = MW_Brain_location\n",
    "    N_locations = len(Brain_location)\n",
    "    test_size = int(0.2 * data_size)\n",
    "    train_size = data_size - test_size\n",
    "    N_data = N_Samples_device[device]\n",
    "    if N_locations > 1:\n",
    "        arr = np.zeros([data_size, N_locations, N_data])\n",
    "    else:\n",
    "        arr = np.zeros([data_size, N_data], dtype = 'int32')\n",
    "    label = np.zeros([data_size], dtype = 'int32')\n",
    "    for i in range(N_locations*data_size):\n",
    "        \n",
    "        temp = infile.readline()\n",
    "        if len(temp) < 10:\n",
    "            break\n",
    "        x = temp.split()\n",
    "        header = x[0:6]\n",
    "        event = int(header[1]) - event_start_point\n",
    "        channel = Brain_location[header[3]]\n",
    "        temp = x[6].split(',')\n",
    "        while len(temp) < N_data:\n",
    "            temp.append('0')\n",
    "        n = int(header[4])\n",
    "#        print(n)\n",
    "        if N_locations > 1:\n",
    "            arr[i//N_locations][channel] = list(map(float,temp))[:N_data]\n",
    "        else:\n",
    "            arr[i//N_locations] = list(map(float,temp))[:N_data]\n",
    "        label[i//N_locations] = n\n",
    "    if split_data == True:\n",
    "        return arr[0:train_size], label[0:train_size], arr[train_size:data_size], label[train_size:data_size]\n",
    "    return arr, label\n",
    "def EPOC_Load_data(infile, data_size = 100, event_start_point = 67635):\n",
    "    N_locations = 14\n",
    "    test_size = int(0.2 * data_size)\n",
    "    train_size = data_size - test_size\n",
    "    N_data = 256\n",
    "    arr = np.zeros([data_size, N_locations, N_data])\n",
    "    label = np.zeros([data_size], dtype = 'int32')\n",
    "    for i in range(N_locations*data_size):\n",
    "        \n",
    "        temp = infile.readline()\n",
    "        if len(temp) < 10:\n",
    "            break\n",
    "        x = temp.split()\n",
    "        header = x[0:6]\n",
    "        event = int(header[1]) - event_start_point\n",
    "        channel = EPOC_Brain_location[header[3]]\n",
    "        temp = x[6].split(',')\n",
    "        while len(temp) < N_data:\n",
    "            temp.append('0')\n",
    "        n = int(header[4])\n",
    "        if(n != -1):\n",
    "            arr[event][channel] = list(map(float,temp))[:N_data]\n",
    "            label[event] = n\n",
    "    return arr,label\n",
    "\n",
    "def get_specific_data(arr, labels, l1, l2):\n",
    "    data = [[] for i in range(10)]\n",
    "    for i in range(len(arr)):\n",
    "        data[labels[i]].append(arr[i].transpose())\n",
    "    length = 0\n",
    "    for i in range(10):\n",
    "        length = length + len(data[i])\n",
    "    length = length // 4\n",
    "    tags = []\n",
    "    content = []\n",
    "    for i in range(length):\n",
    "        if i < len(data[l1]):\n",
    "            tags.append(0)\n",
    "            content.append(data[l1][i])\n",
    "        elif i < len(data[l1]) + len(data[l2]):\n",
    "            tags.append(1)\n",
    "            content.append(data[l2][i - len(data[l1])])\n",
    "        else:\n",
    "            tags.append(2)\n",
    "    for i in range(10):\n",
    "        if i != l1 and i != l2:\n",
    "            for j in range(len(data[i]) // 7):\n",
    "                content.append(data[i][j])\n",
    "    while len(tags) < len(content):\n",
    "      tags.append(2)\n",
    "    return content, tags\n",
    "    \n",
    "Size_of_data = 40000\n",
    "First_Digit = 2\n",
    "Second Digit = 4\n",
    "File_Location = \"EP1.01.txt\"\n",
    "    \n",
    "infile = open(File_Location)\n",
    "data, labels = EPOC_Load_data(infile, data_size = Size_of_data)\n",
    "\n",
    "content, tags = get_specific_data(data, labels, First_Digit, Second Digit)\n",
    "del data\n",
    "del labels\n",
    "data = np.asarray(content)\n",
    "labels = np.asarray(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPWndtqQAeHf"
   },
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sklearn.model_selection** is used to shuffle data and split it into test and train part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EL_fB6V6RHC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(data, labels, test_size=0.20, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network architecure is defined below. The hyper paramers such as number of layers, number of filters in each layer and to name but two, may vary for different digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKDflMrI6GBp"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=np.shape(x_train)[1:]))\n",
    "model.add(tf.keras.layers.LSTM(256, return_sequences=True))  # returns a sequence of vectors of dimension 256 * 14\n",
    "model.add(tf.keras.layers.Dropout(rate = 0.5, noise_shape=None, seed=None))\n",
    "model.add(tf.keras.layers.LeakyReLU())\n",
    "model.add(tf.keras.layers.LSTM(512, return_sequences=True))  # returns a sequence of vectors of dimension 512*14\n",
    "model.add(tf.keras.layers.Dropout(rate = 0.5, noise_shape=None, seed=None))\n",
    "model.add(tf.keras.layers.LeakyReLU())\n",
    "model.add(tf.keras.layers.LSTM(256, return_sequences=True))  # returns a sequence of vectors of dimension 256*14\n",
    "model.add(tf.keras.layers.Dropout(rate = 0.5, noise_shape=None, seed=None))\n",
    "model.add(tf.keras.layers.LeakyReLU())\n",
    "model.add(tf.keras.layers.LSTM(64, return_sequences=True))  # returns a sequence of vectors of dimension 64*14\n",
    "model.add(tf.keras.layers.Dropout(rate = 0.5, noise_shape=None, seed=None))\n",
    "model.add(tf.keras.layers.LeakyReLU())\n",
    "model.add(tf.keras.layers.LSTM(16, return_sequences=True))  # returns a sequence of vectors of dimension 16*14\n",
    "model.add(tf.keras.layers.Dropout(rate = 0.5, noise_shape=None, seed=None))\n",
    "model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "\n",
    "# model.add(tf.keras.layers.Conv1D(128, 11, padding='same', activation='tanh'))\n",
    "# model.add(tf.keras.layers.AveragePooling1D(pool_size=(4)))\n",
    "# model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.Conv1D(256, 11, padding='same', activation='tanh'))\n",
    "# model.add(tf.keras.layers.AveragePooling1D(pool_size=(2)))\n",
    "# model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "# # model.add(tf.keras.layers.BatchNormalization())\n",
    "# # model.add(tf.keras.layers.Conv1D(128, 8, padding='same', activation='elu'))\n",
    "# # model.add(tf.keras.layers.AveragePooling1D(pool_size=(4)))\n",
    "# # model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fAXRp6rH8bmW"
   },
   "outputs": [],
   "source": [
    "print(np.shape(x_train))\n",
    "xx = x_train[:,0,:]\n",
    "print(xx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses TPU to speed up learning procedure. $$\\alpha$$is the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bo23D-I86Wt1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "alpha = 2e-3\n",
    "batch_size = 256\n",
    "num_epochs = 3000\n",
    "\n",
    "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "    model,\n",
    "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "    )\n",
    ")\n",
    "tpu_model.compile(\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=alpha, ),\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['sparse_categorical_accuracy', 'accuracy']\n",
    ")\n",
    "\n",
    "# model.save('CNN1D_model_EPOC.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "filepath = \"./STACKED_LSTM_model_EPOC/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkPointer = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=1, save_best_only=True, save_weights_only=False, mode='auto', monitor='val_acc')\n",
    "logdir=\"./STACKED_LSTM_model_EPOC/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorBoard = tf.keras.callbacks.TensorBoard(log_dir=logdir, write_graph=True, write_grads=False, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "def train_gen(batch_size):\n",
    "  while True:\n",
    "    offset = np.random.randint(0, train_data.shape[0] - batch_size)\n",
    "    yield train_data[offset:offset+batch_size], train_labels[offset:offset + batch_size]\n",
    "    \n",
    "\n",
    "History = tpu_model.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_data = (x_valid, y_valid),\n",
    "    callbacks = [checkPointer,tensorBoard]\n",
    ")\n",
    "import pickle\n",
    "with open('./CNN1D_model_EPOC/trainHistoryDict', 'wb') as file_pi:\n",
    "    pickle.dump(History.history, file_pi)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN-01.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
