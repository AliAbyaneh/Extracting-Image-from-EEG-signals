{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAbyaneh/Extracting-Image-from-EEG-signals/blob/master/CNN_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZBaFQuM51cf",
        "colab_type": "text"
      },
      "source": [
        "This Code is designed to be used in Google Colab and Google Cloud Environment by Ali Abyaneh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC0DUVsO5ucT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "#@title Default title text\n",
        "import os\n",
        "import zipfile\n",
        "cwd = os.getcwd()\n",
        "print(cwd)\n",
        "os.chdir(\"../../\")\n",
        "os.chdir(\"../gdrive/My Drive\")\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO_wgfT_51l0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "!pip install scipy\n",
        "from scipy import signal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_vjkaoN523M",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I7f17Xi52_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "EPOC_Brain_location = {\"AF3\" : 0,\n",
        "                  \"F7\"  : 1,\n",
        "                  \"F3\"  : 2,\n",
        "                  \"FC5\" : 3,\n",
        "                  \"T7\"  : 4,\n",
        "                  \"P7\"  : 5,\n",
        "                  \"O1\"  : 6,\n",
        "                  \"O2\"  : 7,\n",
        "                  \"P8\"  : 8, \n",
        "                  \"T8\"  : 9,\n",
        "                  \"FC6\" : 10,\n",
        "                  \"F4\"  : 11,\n",
        "                  \"F8\"  : 12,\n",
        "                  \"AF4\" : 13}\n",
        "MW_Brain_location = {\"FP1\" : 0}\n",
        "N_Samples_device = {\"MindWave\" : 1024,\n",
        "          \"EPOC\"     : 256 \n",
        "          }\n",
        "def Load_data(infile, device, data_size = 100, event_start_point = 67635, split_data = False):\n",
        "    if device == \"EPOC\":\n",
        "        Brain_location = EPOC_Brain_location\n",
        "    elif device == \"MindWave\":\n",
        "        Brain_location = MW_Brain_location\n",
        "    N_locations = len(Brain_location)\n",
        "    test_size = int(0.2 * data_size)\n",
        "    train_size = data_size - test_size\n",
        "    N_data = N_Samples_device[device]\n",
        "    if N_locations > 1:\n",
        "        arr = np.zeros([data_size, N_locations, N_data])\n",
        "    else:\n",
        "        arr = np.zeros([data_size, N_data], dtype = 'int32')\n",
        "    label = np.zeros([data_size], dtype = 'int32')\n",
        "    for i in range(N_locations*data_size):\n",
        "        \n",
        "        temp = infile.readline()\n",
        "        if len(temp) < 10:\n",
        "            break\n",
        "        x = temp.split()\n",
        "        header = x[0:6]\n",
        "        event = int(header[1]) - event_start_point\n",
        "        channel = Brain_location[header[3]]\n",
        "        temp = x[6].split(',')\n",
        "        while len(temp) < N_data:\n",
        "            temp.append('0')\n",
        "        n = int(header[4])\n",
        "#        print(n)\n",
        "        if N_locations > 1:\n",
        "            arr[i//N_locations][channel] = list(map(float,temp))[:N_data]\n",
        "        else:\n",
        "            arr[i//N_locations] = list(map(float,temp))[:N_data]\n",
        "        label[i//N_locations] = n\n",
        "    if split_data == True:\n",
        "        return arr[0:train_size], label[0:train_size], arr[train_size:data_size], label[train_size:data_size]\n",
        "    return arr, label\n",
        "def EPOC_Load_data(infile, data_size = 100, event_start_point = 67635):\n",
        "    N_locations = 14\n",
        "    test_size = int(0.2 * data_size)\n",
        "    train_size = data_size - test_size\n",
        "    N_data = 256\n",
        "    arr = np.zeros([data_size, N_locations, N_data])\n",
        "    label = np.zeros([data_size], dtype = 'int32')\n",
        "    for i in range(N_locations*data_size):\n",
        "        \n",
        "        temp = infile.readline()\n",
        "        if len(temp) < 10:\n",
        "            break\n",
        "        x = temp.split()\n",
        "        header = x[0:6]\n",
        "        event = int(header[1]) - event_start_point\n",
        "        channel = EPOC_Brain_location[header[3]]\n",
        "        temp = x[6].split(',')\n",
        "        while len(temp) < N_data:\n",
        "            temp.append('0')\n",
        "        n = int(header[4])\n",
        "        if(n != -1):\n",
        "            arr[event][channel] = list(map(float,temp))[:N_data]\n",
        "            label[event] = n\n",
        "    return arr,label\n",
        "\n",
        "def get_specific_data(arr, labels, l1, l2):\n",
        "    data = [[] for i in range(10)]\n",
        "    for i in range(len(arr)):\n",
        "        data[labels[i]].append(arr[i].transpose())\n",
        "    length = 0\n",
        "    for i in range(10):\n",
        "        length = length + len(data[i])\n",
        "    length = length // 4\n",
        "    tags = []\n",
        "    content = []\n",
        "    for i in range(length):\n",
        "        if i < len(data[l1]):\n",
        "            tags.append(0)\n",
        "            content.append(data[l1][i])\n",
        "        elif i < len(data[l1]) + len(data[l2]):\n",
        "            tags.append(1)\n",
        "            content.append(data[l2][i - len(data[l1])])\n",
        "        else:\n",
        "            tags.append(2)\n",
        "    for i in range(10):\n",
        "        if i != l1 and i != l2:\n",
        "            for j in range(len(data[i]) // 7):\n",
        "                content.append(data[i][j])\n",
        "    while len(tags) < len(content):\n",
        "      tags.append(2)\n",
        "    return content, tags\n",
        "    \n",
        "infile = open(\"EP1.01.txt\")\n",
        "data, labels = EPOC_Load_data(infile, data_size = 40000)\n",
        "content, tags = get_specific_data(data[:38000], labels[:38000], 2, 5)\n",
        "del data\n",
        "del labels\n",
        "data = np.asarray(content)\n",
        "labels = np.asarray(tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPWndtqQAeHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EL_fB6V6RHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(data, labels, test_size=0.20, shuffle= True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKDflMrI6GBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.BatchNormalization(input_shape=np.shape(x_train)[1:]))\n",
        "model.add(tf.keras.layers.Conv1D(128, 11, padding='same', activation='tanh'))\n",
        "model.add(tf.keras.layers.AveragePooling1D(pool_size=(4)))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(256, 11, padding='same', activation='tanh'))\n",
        "model.add(tf.keras.layers.AveragePooling1D(pool_size=(2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "\n",
        "# model.add(tf.keras.layers.BatchNormalization())\n",
        "# model.add(tf.keras.layers.Conv1D(128, 8, padding='same', activation='elu'))\n",
        "# model.add(tf.keras.layers.AveragePooling1D(pool_size=(4)))\n",
        "# model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(10))\n",
        "model.add(tf.keras.layers.Activation('elu'))\n",
        "model.add(tf.keras.layers.Dense(3))\n",
        "model.add(tf.keras.layers.Activation('softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAXRp6rH8bmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.shape(x_train))\n",
        "xx = x_train[:,0,:]\n",
        "print(xx.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo23D-I86Wt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    )\n",
        ")\n",
        "tpu_model.compile(\n",
        "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=2e-3, ),\n",
        "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "    metrics=['sparse_categorical_accuracy', 'accuracy']\n",
        ")\n",
        "\n",
        "# model.save('CNN1D_model_EPOC.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "filepath = \"./CNN1D_model_EPOC_s25/weights25-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkPointer = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=1, save_best_only=True, save_weights_only=False, mode='auto', monitor='val_acc')\n",
        "logdir=\"./CNN1D_model_EPOC_s25/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorBoard = tf.keras.callbacks.TensorBoard(log_dir=logdir, write_graph=True, write_grads=False, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
        "def train_gen(batch_size):\n",
        "  while True:\n",
        "    offset = np.random.randint(0, train_data.shape[0] - batch_size)\n",
        "    yield train_data[offset:offset+batch_size], train_labels[offset:offset + batch_size]\n",
        "    \n",
        "\n",
        "History = tpu_model.fit(\n",
        "    x = x_train,\n",
        "    y = y_train,\n",
        "    batch_size = 256,\n",
        "    epochs=3000,\n",
        "    validation_data = (x_valid, y_valid),\n",
        "    callbacks = [checkPointer,tensorBoard]\n",
        ")\n",
        "import pickle\n",
        "with open('./CNN1D_model_EPOC/trainHistoryDict', 'wb') as file_pi:\n",
        "    pickle.dump(History.history, file_pi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcxb7uXqxRD5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpEC4agxxRQW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEPnbXMcJhyx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ284Ch9xt0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}